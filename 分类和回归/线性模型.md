## 线性模型

​	给定包含$d$个属性的样本$\bold{x}=(x_1;x_2;...x_d)$，线性模型学习一个【通过各个属性的线性组合来进行预测】的函数，即
$$
f(\bold x)=\bold{w^Tx}+b
$$
学习到$\bold w$和$b$之后，即可确定一个线性模型。线性模型的优势在于参数$\bold w$直观地表达了样本的各个属性对最终预测结果的重要性（正负，大小），因此线性模型具有很好的可解释性。



### 1. 单变量线性回归

​	线性回归模型试图根据数据集，学习一个【预测实数值输出】的线性模型。先考虑样本中只有一个属性，即$d=1$的情况，用$(x_i,y_i)$表示一个样本，则线性回归试图学习参数$w$和$b$，使得
$$
f(x_i)=wx_i+b
$$
尽可能与$y_i$接近。衡量接近的方法是计算$f(x)$与$y$的均方误差（MSE），即：
$$
\sum_{i=1}^N(y_i-f(x_i))^2
$$
令MSE最小化，可得：
$$
\begin{align}
\frac{\partial\sum_{i=1}^N(y_i-wx_i-b)^2}{\partial w}=&\frac{\partial\sum_{i=1}^N(w^2x_i^2-2wx_i(y_i-b)+(y_i-b)^2)}{\partial w} \\
=&\frac{\partial w^2\sum_{i=1}^Nx_i^2}{\partial w}-2\sum_{i=1}^N x_i(y_i-b) \\
=&2w\sum_{i=1}^Nx_i^2-2\sum_{i=1}^N x_iy_i + 2b\sum_{i=1}^Nx_i
\end{align}
$$

$$
\begin{align}
\frac{\partial\sum_{i=1}^N(y_i-wx_i-b)^2}{\partial b}=&\frac{\partial\sum_{i=1}^N(w^2x_i^2-2wx_i(y_i-b)+(y_i-b)^2)}{\partial b} \\
=&\frac{\partial2b\sum_{i=1}^Nwx_i-2b\sum_{i=1}^Ny_i+Nb^2}{\partial b} \\
=&2Nb+2\sum_{i=1}^Nwx_i-2\sum_{i=1}^Ny_i
\end{align}
$$

令对$b$的导数为0，可得：
$$
b^*=\frac{1}{N}\sum_{i=1}^N(y_i-wx_i) =\bar y-w\bar x
$$
将结果代入对$w$的导数，并令其为0，可得：
$$
w\sum_{i=1}^Nx_i^2-\sum_{i=1}^Nx_iy_i+\bar y\sum_{i=1}^Nx_i-w\bar x\sum_{i=1}^Nx_i=0
$$
求解可得：
$$
\begin{align}
w^*(\sum_{i=1}^Nx_i^2-\bar x\sum_{i=1}^Nx_i)=&\sum_{i=1}^Nx_iy_i-\bar y\sum_{i=1}^Nx_i \\
w^*=&\frac{\sum_{i=1}^Nx_iy_i-\bar y\sum_{i=1}^Nx_i}{\sum_{i=1}^Nx_i^2-\bar x\sum_{i=1}^Nx_i} \\
=&\frac{\sum_{i=1}^Nx_iy_i-N\bar x\bar y}{\sum_{i=1}^Nx_i^2-2N\bar x^2+N\bar x^2} \\
=&\frac{\sum_{i=1}^Nx_iy_i-\sum_{i=1}^Nx_i\bar y-\sum_{i=1}^N\bar xy_i+\sum_{i=1}^N\bar x\bar y}{\sum_{i=1}^Nx_i^2-2\bar x\sum_{i=1}^Nx_i+\sum_{i=1}^N\bar x^2} \\
=&\frac{\sum_{i=1}^N(x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^N(x_i-\bar x)^2}
\end{align}
$$
这是样本中只有一个属性的情况下的线性回归。



### 2.多变量线性回归

​	当样本包含多个属性时，此时，我们试图学习到参数$\bold w$和$b$，其中$\bold w$是维度等于属性数目的列向量，$b$是标量，使得
$$
f(\bold x_i)=\bold{w^Tx}_i+b
$$
与$y_i$尽可能接近。为简化表述，将$b$吸收到$\bold w$中，即$\bold w\leftarrow(\bold w;b)$，数据集表示为一个行数等于样本数，列数为属性数目+1的矩阵$\bold X$，label写成向量形式$\bold y$，则有：
$$
\bold {y=Xw}
$$
均方误差为$\bold{(y-Xw)^T(y-Xw)}$，对$\bold w$求偏导数，有：
$$
\begin{align}
\bold{\frac{\partial {(y-Xw)^T(y-Xw)}}{\partial w}}=&\bold{\frac{\partial((y-Xw)^Ty-(y-Xw)^TXw)}{\partial w}} \\
=& \bold{\frac{\partial (y^Ty-w^TX^Ty-y^TXw+w^TX^TXw)}{\partial w}}
\end{align}
$$
由于$\bold{w^TX^Ty=(y^TXw)^T}$，且二者均为标量，故$\bold{w^TX^Ty=y^TXw}$。上式可变形为：
$$
\begin{align}
\bold{\frac{\partial (y^Ty-2w^TX^Ty+w^TX^TXw)}{\partial w}=-2X^Ty+2X^TXw}
\end{align}
$$
若$\bold {X^TX}$为满秩矩阵或正定矩阵，令上式为0，可得：
$$
\begin{align}
\bold{X^TXw}=&\bold{X^Ty} \\
\bold w=&\bold {(X^TX)^{-1}X^Ty}
\end{align}
$$

若$\bold {X^TX}$不满秩，则由多个能使均方误差最小的$\bold w$值，此时需要引入正则化项，常用的正则化项包括Ridge和Lasso两种。

​	Ridge正则化在均方误差的基础上增加一个正则化项$\lambda \bold {w^Tw}$，此时，误差函数对$\bold w$的梯度变为:
$$
\bold {-2X^Ty+2X^TXw+2\lambda w}
$$
令梯度为0，可得：
$$
\bold{w=(\lambda I+X^TX)^{-1}X^Ty}
$$
其中，$\bold I$是形状与$\bold {X^TX}$相同的单位矩阵。

​	Lasso正则化在均方误差的基础上增加一个正则化项$\lambda\sum_j |w_j|$。当$\lambda$足够大时，Lasso正则化会导致向量$\bold w$中某些维度的取值为0，即产生稀疏的向量。这是因为Lasso正则化约束了向量$\bold w$中各维度取值的绝对值之和，从PRML146页的图中，可以观察到误差函数的等高线图将与$\bold w$的取值相交于坐标轴上。由于Lasso正则化加入的正则化项不可导，无法直接求解或使用梯度下降等方法进行优化，一般可以使用坐标轴下降法进行优化，即在每一轮迭代中，在当前点处固定其它坐标轴方向，沿着某一个坐标轴方向进行一维搜索，当所有的坐标轴上都达到收敛时，向量$\bold w$的取值即为所需结果。



### 3. Logistic回归

​	Logistic回归是一种用于二分类任务的线性模型。二分类任务中，输出$y\in \{0,1\}$ ，因此，需要将线性模型$\bold{w^Tx}+b$ 的输出转换成0或1。Logistic回归通过sigmoid函数：
$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$
实现实数值到(0,1)区间的转换。 即：
$$
\bold{y}=\frac{1}{1+e^{\bold{-w^Tx}+b}}
$$
如果令$\bold y$表示样本$\bold x$作为正样本的可能性，则有：
$$
\bold{w^Tx}+b=ln\frac{y}{1-y}
$$
即通过线性模型来拟合样本$\bold x$作为正样本的相对可能性。 

​	求解$\bold w$和$b$的取值可以使用最大似然法完成。给定训练数据集$\{x_i,y_i\}$，其对数似然为$\sum ln p(y_i|x_i,\bold {w}, b)$。如果用$\beta=(\bold w, b)$,$\hat{x}=(\bold x, 1), \sigma=\frac{1}{1+e^{-\beta^T\hat x}}$，则有
$$
\begin{align}
p(y_i|x_i,\bold w, b)=&\sigma^{y_i}(1-\sigma)^{1-y_i} \\
ln(\sigma^{y_i}(1-\sigma)^{1-y_i})=&y_iln(\sigma)+(1-y_i)ln(1-\sigma) \\ \\

NLL =& -\sum ln(\sigma^{y_i}(1-\sigma)^{1-y_i}) \\
\frac{\partial NLL}{\partial \beta}=&\sum\{y_i*\frac{1}{\sigma}*\sigma(1-\sigma)\hat x_i+(1-y_i)*\frac{1}{1-\sigma}*-\sigma(1-\sigma)\hat x_i\} \\
=&-\sum\{y_i(1-\sigma)\hat x_i-(1-y_i)\sigma \hat x_i\} \\
=&-\sum(y_i\hat x_i-\sigma y_i\hat x_i-\sigma \hat x_i+\sigma y_i\hat x_i) \\
=&\sum\{(\sigma-y_i)\hat x_i\}

\end{align}
$$
此处$NLL$指Negative Log-Likelihood。因此，可以据此构造一个逐步更新$\beta$的算法，在每一步，均利用上式的梯度更新$\beta$的取值。



### 4. 线性判别分析

​	线性分类模型可以被看成是一种降维。以二分类为例，假设输入向量$\bold x$是一个$d$维向量，分类模型将通过$y=\bold {w^Tx}$其降维到1维，并在$y$上设置一个阈值，当$y$大于这个阈值时判为正类，否则判为负类。将高位向量降维到1为必然会带来一定的信息损失，如在高维空间中可分的两类数据点降维到1维后，有可能出现相互重叠的情况。因此，我们需要选择合适的$\bold w$值，使得两类数据点被降维后最大程度地可分。设$C_1$表示第一类数据点，共有$N_1$个，$C_2$表示第二类数据点，共有$N_2$个，两类数据点的均值用$\bold m_1$和$\bold m_2$表示。

​	一种最简单的度量被降维后的数据的可分情况的方法是最大化其均值被降维后的距离，即：
$$
m_2-m_1=\bold {w^T(m_2-m_1)}
$$
​	由于增大$\bold w$的取值，可以让上式变得任意大，因此需要将$\bold w$约束成单位向量。使用拉格朗日乘子法可得$\bold {w \propto (m_2 - m_1)}$。

​	但是，当数据点的协方差矩阵的对角性很差时，按上述$\bold w$降维后，两类数据仍可能有明显的重叠（图见PRML188页）。因此，除了最大化均值被降维后的距离，还需要让类内各数据点被降维后的结果尽量接近，从而减少重叠部分。

​	对于第$k$类数据，映射后的类内方差可以表示为：
$$
s^2_k=\sum_{n \in C_k}(y_n-m_k)^2
$$
​	其中$y_n=\bold {w^Tx_n}$ 。因此，对于二分类问题，我们需要最大化：
$$
J(\bold w)=\frac{(m_2-m_1)^2}{s^2_1+s^2_2}
$$
​	分子：
$$
\begin{align}
(m_2-m_1)^2=&(\bold {w^T(m_2-m_1)})^2 \\
=& \bold {w^T(m_2-m_1)(m_2-m_1)^Tw} \\
=& \bold {w^TS_Bw}
\end{align}
$$
​	其中，$\bold {S_B=(m_2-m_1)(m_2-m_1)^T}$

​	分母：
$$
\begin{align}
s^2_1+s^2_2=&\bold{\sum(w^T(x-m_1))^2+\sum(w^T(x-m_2))^2} \\
=&\bold{\sum w^T(x-m_1)(x-m_1)^Tw + \sum w^T(x-m_2)(x-m_2)^Tw} \\
=&\bold{w^TS_Ww}
\end{align}
$$
​	其中，$\bold{S_w=\sum(x-m_1)(x-m_1)^T + \sum (x-m_2)(x-m_2)^T}$。

​	令$J(\bold w)$对$\bold w$的导数为0，可得：
$$
\bold {(w^TS_Bw)S_ww=(w^TS_Ww)S_Bw}
$$
​	由于$\bold {S_Bw}$方向始终与$\bold {(m_2-m_1)}$相同，令$\bold{S_Bw=\lambda(m_2-m_1)}$，两边同时乘以$\bold {S^{-1}_W}$可得:
$$
\bold {w \propto S^{-1}_W(m_2-m_1)}
$$
​	如果类内方差是各向同性的，即$\bold {S_W}$为单位矩阵，那么$\bold w$就正比于两类均值向量之差。

​	注意，这不能直接求解出分类结果，而只能确定一个将高维样本映射到一维数值的方案，还需要结合其它算法来求出阈值。

​	对于多分类情况：设有$k$个分类，对于输入的列向量$\bold x$，可以使用一个$k$列的矩阵$\bold W$计算对应的向量$\bold y$：
$$
\bold {y=W^Tx}
$$
同二分类情况，用$\bold {S_k}$表示第$k$类的类内协方差，则总类内协方差为：
$$
\bold{S_W=\sum S_k}
$$
样本总协方差为:
$$
\bold {S_T=\sum(x_n-m)(x_n-m)^T}
$$
其中，$\bold m$是所有样本的均值向量。总协方差可以分解成类内协方差和类间协方差，即:
$$
\bold{S_T=S_W+S_B}
$$
因此有：
$$
\bold {S_B=\sum_k N_k(m_k-m)(m_k-m)^T}
$$
其中$\bold{N_k}$是第$k$类的样本数。

​	我们需要构造一个标量，使得在类间协方差较大，同时类内协方差较小的时候，这个标量的值较大。一种构造方法是：
$$
J(\bold w)=Tr\{\bold {(WS_WW^T)^{-1}(WS_BW^T)}\}
$$
结论表明，此时权重向量可以由矩阵$\bold {S^{-1}_WS_B}$的最大的$D$个特征值对应的特征向量确定。